# ============================================================================
# TTS/STT Pipeline Configuration Template
# ============================================================================
# Copy this file to .env and configure with your actual values:
#   Windows: copy .env.example .env
#   Linux/Mac: cp .env.example .env
# ============================================================================

# ----------------------------------------------------------------------------
# LLM Configuration (REQUIRED)
# ----------------------------------------------------------------------------
# LLM API endpoint - Ollama (cloud or local)
# Cloud: https://ollama.com (requires API key from https://ollama.com)
# Local: http://localhost:11434 (requires Ollama installed locally)
LLM_HOST=http://localhost:11434

# Model to use for chat completions
# Cloud models: glm-4.7:cloud, qwen:latest, llama3.2:latest
# Local models: llama3.2, mistral, qwen2.5, etc.
# See: https://ollama.com/search
LLM_MODEL=llama3.2

# Device to run LLM on:
#   auto - Automatically choose best available device
#   cuda - Use NVIDIA GPU (faster, requires CUDA)
#   cpu - Use CPU (slower but works everywhere)
LLM_DEVICE=auto

# Keep-alive duration for models (in seconds)
# -1 = keep loaded forever (recommended for frequent use)
# 0 = unload immediately after each request (LOW_VRAM_MODE)
# >0 = unload after specified seconds of inactivity
LLM_KEEP_ALIVE=-1

# ----------------------------------------------------------------------------
# Whisper STT (Speech-to-Text) Configuration
# ----------------------------------------------------------------------------
# Model name or local path to model directory:
#   tiny.en, base.en - Fast, lower accuracy
#   small.en, distil-small.en - Balanced
#   medium.en, distil-medium.en - Better accuracy (recommended)
#   large-v2, large-v3 - Best quality, slower
#   C:\Users\username\ (local path)
WHISPER_MODEL=distil-medium.en

# Device to run Whisper on:
#   cuda - Use NVIDIA GPU (faster, requires CUDA)
#   cpu - Use CPU (slower but works everywhere)
WHISPER_DEVICE=cuda

# Compute type for inference:
#   float16 - Fast, good quality (GPU only)
#   float32 - Slower, slightly better (GPU/CPU)
#   int8 - Fastest, lower quality (CPU/GPU)
WHISPER_COMPUTE=float16

# ----------------------------------------------------------------------------
# Server Configuration
# ----------------------------------------------------------------------------
# Host to bind the backend server to
#   0.0.0.0 - Listen on all network interfaces
#   127.0.0.1 - Only listen on localhost
BACKEND_HOST=127.0.0.1

# Port to run the backend server on
BACKEND_PORT=8000

# WebSocket Configuration
# Ping interval (seconds) - how often to send ping to keep connection alive
WS_PING_INTERVAL=30

# Ping timeout (seconds) - how long to wait for pong response
WS_PING_TIMEOUT=60

# Keep-alive timeout (seconds) - maximum idle time before closing connection
WS_KEEPALIVE_TIMEOUT=300

# ----------------------------------------------------------------------------
# TTS (Text-to-Speech) Configuration
# ----------------------------------------------------------------------------
# Options: "piper", "chatterbox", "soprano"
TTS_ENGINE=piper

# Piper TTS - default, lightweight, fast
# Download voices from: https://huggingface.co/rhasspy/piper-voices/tree/main
# Place voice files (.onnx and .json) in: src/models/voices/pipertts/
# Example voices:
#   - en_US-lessac-medium
#   - en_US-amy-medium
#   - en_GB-alan-medium
PIPER_USE_CUDA=true

# Chatterbox TTS Configuration (recommended, expressive, requires installation)
# Install: pip install chatterbox-tts
CHATTERBOX_MODEL_TYPE=turbo
CHATTERBOX_DEVICE=cuda
CHATTERBOX_EXAGGERATION=0.5
CHATTERBOX_CFG_WEIGHT=0.5

# Soprano TTS Configuration (fast, lightweight, requires installation)
# Install: pip install soprano-tts
SOPRANO_BACKEND=auto
SOPRANO_DEVICE=cuda
SOPRANO_OFFLINE_MODE=false

# ----------------------------------------------------------------------------
# Image Generation Configuration
# ----------------------------------------------------------------------------
# Enable/disable image generation feature
IMAGEGEN_ENABLED=true

# Image generation model type:
#   - diffusion: Traditional diffusion models (Stable Diffusion, etc.)
#   - qwen: Qwen Image Edit model (supports both generation and editing) TODO
IMAGEGEN_MODEL_TYPE=diffusion

# HuggingFace model ID or local path to diffusion model directory (for diffusion type)
# Popular options:
#   - prompthero/openjourney (default, fast, small VRAM)
#   - runwayml/stable-diffusion-v1-5 (general purpose)
#   - stabilityai/stable-diffusion-2-1 (better quality)
#   - hakurei/waifu-diffusion (anime style)
#   - C:\Users\username\ (local path)
IMAGEGEN_MODEL=prompthero/openjourney

# Device for image generation (cuda or cpu)
IMAGEGEN_DEVICE=cuda

# Image dimensions (512x512 recommended for real-time, requires ~6GB VRAM)
# For 1024x1024, need 10GB+ VRAM
IMAGEGEN_WIDTH=512
IMAGEGEN_HEIGHT=512

# Inference steps (20-30 for speed, 40-50 for quality, 4 for Qwen Lightning models)
IMAGEGEN_STEPS=30

# Guidance scale (7-9 recommended for diffusion, 1.0 for Qwen Lightning models)
IMAGEGEN_GUIDANCE=7.5

# Edit strength for image editing (0.0-1.0, higher = more transformation)
IMAGEGEN_STRENGTH=0.8

# LoRA Configuration (for diffusion models only - character consistency and style)
IMAGEGEN_LORA_ENABLED=true
# Path to LoRA file (.safetensors)
IMAGEGEN_LORA_PATH=models/imagegen_loras/woman037-zimage.safetensors
# LoRA weight/strength (0.0-1.0, higher = stronger effect)
IMAGEGEN_LORA_WEIGHT=0.8

# ----------------------------------------------------------------------------
# Image Explainer Configuration (Vision-Language Model)
# ----------------------------------------------------------------------------
# Enable/disable image explanation feature
IMAGEEXPLAINER_ENABLED=true

# HuggingFace model ID or local path to model directory
# Options:
#   - huihui-ai/Huihui-Qwen3-VL-2B-Instruct-abliterated (default, 2B params)
#   - huihui-ai/Huihui-Qwen3-VL-4B-Instruct-abliterated (better quality, 4B params)
#   - C:\Users\username\ (local path)
IMAGEEXPLAINER_MODEL=huihui-ai/Huihui-Qwen3-VL-2B-Instruct-abliterated

# Device for image explanation (auto, cuda, or cpu)
# auto - automatically chooses best available device
IMAGEEXPLAINER_DEVICE=auto

# Maximum tokens to generate for image descriptions
IMAGEEXPLAINER_MAX_TOKENS=256

# ----------------------------------------------------------------------------
# Low VRAM Mode
# ----------------------------------------------------------------------------
# Enable automatic model unloading after each use to conserve VRAM
# Recommended for GPUs with <12GB VRAM
LOW_VRAM_MODE=false
