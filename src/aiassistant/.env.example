# ============================================================================
# TTS/STT Pipeline Configuration Template
# ============================================================================
# Copy this file to .env and configure with your actual values:
#   Windows: copy .env.example .env
#   Linux/Mac: cp .env.example .env
# ============================================================================

# ----------------------------------------------------------------------------
# LLM Configuration (REQUIRED)
# ----------------------------------------------------------------------------
# LLM API endpoint - Ollama (cloud or local)
# Cloud: https://ollama.com (requires API key from https://ollama.com)
# Local: http://localhost:11434 (requires Ollama installed locally)
LLM_HOST=http://localhost:11434

# Model to use for chat completions
# Cloud models: glm-4.7:cloud, qwen:latest, llama3.2:latest
# Local models: llama3.2, mistral, qwen2.5, etc.
# See: https://ollama.com/search
LLM_MODEL=llama3.2

# Device to run LLM on:
#   auto - Automatically choose best available device
#   cuda - Use NVIDIA GPU (faster, requires CUDA)
#   cpu - Use CPU (slower but works everywhere)
LLM_DEVICE=auto

# Keep-alive duration for models (in seconds)
# -1 = keep loaded forever (recommended for frequent use)
# 0 = unload immediately after each request (LOW_VRAM_MODE)
# >0 = unload after specified seconds of inactivity
LLM_KEEP_ALIVE=-1

# ----------------------------------------------------------------------------
# Whisper STT (Speech-to-Text) Configuration
# ----------------------------------------------------------------------------
# Model name or local path to model directory:
#   tiny.en, base.en - Fast, lower accuracy
#   small.en, distil-small.en - Balanced
#   medium.en, distil-medium.en - Better accuracy (recommended)
#   large-v2, large-v3 - Best quality, slower
#   C:\Users\username\ (local path)
WHISPER_MODEL=distil-medium.en

# Device to run Whisper on:
#   cuda - Use NVIDIA GPU (faster, requires CUDA)
#   cpu - Use CPU (slower but works everywhere)
WHISPER_DEVICE=cuda

# Compute type for inference:
#   float16 - Fast, good quality (GPU only)
#   float32 - Slower, slightly better (GPU/CPU)
#   int8 - Fastest, lower quality (CPU/GPU)
WHISPER_COMPUTE=float16

# ----------------------------------------------------------------------------
# Server Configuration
# ----------------------------------------------------------------------------
# Host to bind the backend server to
#   0.0.0.0 - Listen on all network interfaces
#   127.0.0.1 - Only listen on localhost
BACKEND_HOST=127.0.0.1

# Port to run the backend server on
BACKEND_PORT=8000

# WebSocket Configuration
# Ping interval (seconds) - how often to send ping to keep connection alive
WS_PING_INTERVAL=30

# Ping timeout (seconds) - how long to wait for pong response
WS_PING_TIMEOUT=60

# Keep-alive timeout (seconds) - maximum idle time before closing connection
WS_KEEPALIVE_TIMEOUT=300

# ----------------------------------------------------------------------------
# TTS (Text-to-Speech) Configuration
# ----------------------------------------------------------------------------
# TTS Engine Selection: "piper", "chatterbox", or "soprano"
# - piper: Default, lightweight, fast, no dependencies
# - chatterbox: Expressive, voice cloning, paralinguistic tags [laugh], [sigh], etc.
# - soprano: Ultra-fast (2000Ã— real-time), high-fidelity 32kHz, minimal latency
TTS_ENGINE=piper

# ---------- Piper TTS Configuration (Default) ----------
# Lightweight, fast, ONNX-based TTS
# Download voices from: https://huggingface.co/rhasspy/piper-voices/tree/main
# Place voice files (.onnx and .json) in: src/models/voices/pipertts/
# Example voices: en_US-lessac-medium, en_GB-jenny_dioco-medium, hi_IN-priyamvada-medium
PIPER_USE_CUDA=true

# ---------- Chatterbox TTS Configuration ----------
# Zero-shot voice cloning with expressive paralinguistic features
# Install: pip install chatterbox-tts
# Model types:
#   - turbo: 350M params, fastest, supports [laugh], [chuckle], [cough], [gasp], etc.
#   - standard: 500M params, English only, tunable exaggeration & CFG
#   - multilingual: 500M params, 23+ languages
CHATTERBOX_MODEL_TYPE=turbo
CHATTERBOX_DEVICE=cuda
# Reference audio directory for voice cloning (optional)
# CHATTERBOX_REF_AUDIO_DIR=src/aiassistant/voices/chatterbox_refs
# Default reference audio (optional - Chatterbox works without it)
# CHATTERBOX_DEFAULT_REF_AUDIO=src/aiassistant/voices/chatterbox_refs/Amelia_price.wav
# Exaggeration (0.0-1.0+, higher = more expressive/dramatic speech)
CHATTERBOX_EXAGGERATION=0.5
# CFG weight (0.0-1.0, lower = slower, more deliberate pacing)
CHATTERBOX_CFG_WEIGHT=0.5

# ---------- Soprano TTS Configuration ----------
# Ultra-lightweight (80M params), ultra-fast TTS with streaming support
# Install: pip install soprano-tts
# Requires: CUDA GPU (CPU support coming soon)
# Backend options:
#   - auto: Uses LMDeploy if available, falls back to transformers
#   - lmdeploy: Fastest inference backend
#   - transformers: Standard PyTorch backend
SOPRANO_BACKEND=auto
SOPRANO_DEVICE=cuda
# Local model directory for caching models (optional)
# SOPRANO_MODEL_DIR=src/aiassistant/models/soprano
# Cache size (MB) - higher = faster but more VRAM (default: 10)
SOPRANO_CACHE_SIZE_MB=10
# Decoder batch size - higher = faster but more VRAM (default: 1)
SOPRANO_DECODER_BATCH_SIZE=1
# Sampling parameters for voice variation
SOPRANO_TEMPERATURE=0.7
SOPRANO_TOP_P=0.95
SOPRANO_REPETITION_PENALTY=1.0

# ----------------------------------------------------------------------------
# Image Generation Configuration
# ----------------------------------------------------------------------------
# Enable/disable image generation feature
IMAGEGEN_ENABLED=true

# Image generation model type:
#   - diffusion: Traditional diffusion models (Stable Diffusion, etc.)
#   - qwen: Qwen Image Edit model (supports both generation and editing) TODO
IMAGEGEN_MODEL_TYPE=diffusion

# HuggingFace model ID or local path to diffusion model directory (for diffusion type)
# Popular options:
#   - prompthero/openjourney (default, fast, small VRAM)
#   - runwayml/stable-diffusion-v1-5 (general purpose)
#   - stabilityai/stable-diffusion-2-1 (better quality)
#   - hakurei/waifu-diffusion (anime style)
#   - C:\Users\username\ (local path)
IMAGEGEN_MODEL=prompthero/openjourney

# Device for image generation (cuda or cpu)
IMAGEGEN_DEVICE=cuda

# Image dimensions (512x512 recommended for real-time, requires ~6GB VRAM)
# For 1024x1024, need 10GB+ VRAM
IMAGEGEN_WIDTH=512
IMAGEGEN_HEIGHT=512

# Inference steps (20-30 for speed, 40-50 for quality, 4 for Qwen Lightning models)
IMAGEGEN_STEPS=30

# Guidance scale (7-9 recommended for diffusion, 1.0 for Qwen Lightning models)
IMAGEGEN_GUIDANCE=7.5

# Edit strength for image editing (0.0-1.0, higher = more transformation)
IMAGEGEN_STRENGTH=0.8

# LoRA Configuration (for diffusion models only - character consistency and style)
IMAGEGEN_LORA_ENABLED=true
# Path to LoRA file (.safetensors)
IMAGEGEN_LORA_PATH=models/imagegen_loras/woman037-zimage.safetensors
# LoRA weight/strength (0.0-1.0, higher = stronger effect)
IMAGEGEN_LORA_WEIGHT=0.8

# ----------------------------------------------------------------------------
# Image Explainer Configuration (Vision-Language Model)
# ----------------------------------------------------------------------------
# Enable/disable image explanation feature
IMAGEEXPLAINER_ENABLED=true

# HuggingFace model ID or local path to model directory
# Options:
#   - huihui-ai/Huihui-Qwen3-VL-2B-Instruct-abliterated (default, 2B params)
#   - huihui-ai/Huihui-Qwen3-VL-4B-Instruct-abliterated (better quality, 4B params)
#   - C:\Users\username\ (local path)
IMAGEEXPLAINER_MODEL=huihui-ai/Huihui-Qwen3-VL-2B-Instruct-abliterated

# Device for image explanation (auto, cuda, or cpu)
# auto - automatically chooses best available device
IMAGEEXPLAINER_DEVICE=auto

# Maximum tokens to generate for image descriptions
IMAGEEXPLAINER_MAX_TOKENS=256

# ----------------------------------------------------------------------------
# Low VRAM Mode
# ----------------------------------------------------------------------------
# Enable automatic model unloading after each use to conserve VRAM
# Recommended for GPUs with <12GB VRAM
LOW_VRAM_MODE=false
